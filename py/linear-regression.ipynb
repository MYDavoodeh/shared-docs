{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comment me once downloaded\n",
    "import quandl\n",
    "quandl.get('WIKI/GOOGL').to_csv('wikigoogl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('wikigoogl.csv')\n",
    "df['Delta'] = (df['Adj. High'] - df['Adj. Close']) / df['Adj. Close']\n",
    "df['Daily Delta'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open']\n",
    "\n",
    "df = df[['Adj. Close', 'Delta', 'Daily Delta', 'Adj. Volume']]\n",
    "\n",
    "forcast = int(ceil(.01*len(df))) # gets number of wanted data (forcast) based on the last 1% of data\n",
    "\n",
    "df['label'] = df['Adj. Close'].shift(-forcast) # shifts forcast# up and assigns it as a label(what happens in 30 days for our data)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = np.array(df.drop('label', 'columns'))\n",
    "y = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# X = preprocessing.scale(X)\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# TODO implement train_test_split by yourself\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "class MyLinearRegression:\n",
    "    def __init__(self, biased=False):\n",
    "        self.biased = biased\n",
    "        self.Theta = np.nan\n",
    "        self.X = np.nan\n",
    "        self.y = np.nan\n",
    "        self.mse_history = np.nan\n",
    "\n",
    "    def h(self, X):\n",
    "        return X @ self.Theta\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"Calculate (H - y, Mean Square Error)\"\"\"\n",
    "        gradient = self.h(X) - y\n",
    "        return (gradient, np.mean((gradient)**2) / 2)\n",
    "         \n",
    "    def __manage(self, X):\n",
    "        # Manage dimensions, bias and stuff\n",
    "        # If biased, leave it. If not biased, bias it.\n",
    "        X = np.array(X)\n",
    "        if not self.biased and X.ndim == 1:\n",
    "            X = np.transpose([X])\n",
    "        X = np.hstack((np.transpose([np.ones(len(X))]), X))\n",
    "        return X\n",
    "    \n",
    "    def __descent(self, alpha=.0005, min_change=10**-3, max_iterations=100000):\n",
    "        \"\"\"Minimize cost function via minor tweaks.\n",
    "        loss = h - y\n",
    "        First it calculates the loss, then it calculates theta - alpha * X' * loss / m\n",
    "        Then it calculates the gradient = X'.loss / m\n",
    "        At last it updates the Theta like Theta = Theta - alpha * gradient\n",
    "        Keeps doing the process above until convergence is declared (improvement is less than min_change) or\n",
    "        the number of iterations is more than max_iterations\"\"\"\n",
    "        iterations = 0\n",
    "        XT = self.X.T\n",
    "        mse_history = np.array([np.inf, np.inf])\n",
    "        mse = np.inf\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        while iterations < max_iterations and not mse_history[-2] - mse_history[-1] < min_change:\n",
    "            mse_history = np.append(mse_history, mse)\n",
    "            gradient, mse = self.loss(self.X, self.y)\n",
    "            self.Theta = self.Theta - alpha * (XT @ gradient / len(self.X))\n",
    "            iterations += 1\n",
    "        self.mse_history = mse_history\n",
    "        return mse\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = self.__manage(X)\n",
    "        self.y = y\n",
    "        self.Theta = np.ones(self.X.shape[1])\n",
    "        return self.__descent()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # In case the number of features is not equal to the number of features used in training, it will add zeros.\n",
    "        X = self.__manage(X)\n",
    "        extra_col = self.X.shape[1] - X.shape[1]\n",
    "        if 0 < extra_col:\n",
    "            zs = np.zeros((len(X), extra_col), dtype=X.dtype)\n",
    "            X = np.hstack((X, zs))\n",
    "        # Calculate values for X\n",
    "        return self.h(X)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"An abstract way of scoring\"\"\"\n",
    "        # TODO implement something real\n",
    "        X = self.__manage(X)\n",
    "        ssr = 0\n",
    "        sst = 0\n",
    "        for i in range(0,len(X)):\n",
    "            sst += (y[i] - np.mean(y))**2\n",
    "            ssr += (y[i] - self.h(X)[i])**2\n",
    "        return 1 - (ssr/sst)\n",
    "        # return -self.loss(X, y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Execute and draw (represent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MyLinearRegression()\n",
    "def represent(i):\n",
    "    clf.fit(X_train[:, i], y_train)\n",
    "    # TODO Plot for 2d samples\n",
    "    try:\n",
    "        if 0 <= i:\n",
    "            # Plot for 1d samples\n",
    "            plt.scatter(X_train[:, i], clf.y)\n",
    "            plt.plot(np.linspace(-5, 5, 100), clf.Theta[0] + clf.Theta[1] * np.linspace(-5, 5, 100), '-r')\n",
    "            plt.show()\n",
    "            # Plot for comparing 1d samples and predictions\n",
    "            pred_y = clf.predict(X_test[:, i])\n",
    "            plt.scatter(X_test[:, i], y_test)\n",
    "            plt.scatter(X_test[:, i], pred_y)\n",
    "            plt.show()\n",
    "    except TypeError: pass\n",
    "\n",
    "    # Cost plot\n",
    "    plt.plot(clf.mse_history)\n",
    "    plt.show()\n",
    "    score = clf.score(X_test[:, i], y_test)\n",
    "    print(\"The score of your model is: {}\".format(score))\n",
    "    return score\n",
    "    \n",
    "print(\"Printing learning curve for all the features\")\n",
    "my_score = represent(...) # For all features use ...\n",
    "# Only feature 0 is really good for testing with such a basic implementation\n",
    "print(\"Now printing all features one by one (why not?)\")\n",
    "for i in range(0, X_train.shape[1]):\n",
    "    represent(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compare to Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score using all features was 0.9776987081080322 and Sklearn's was 0.9778146260325669 which is -0.000001% different\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "sk_score = clf.score(X_test, y_test)\n",
    "\n",
    "print('Your score using all features was {} and Sklearn\\'s was {} which is {:f}% different'.format(my_score, sk_score, delta_score, -(my_score - sk_score) / my_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "linear-regression.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
